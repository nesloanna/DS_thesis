{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/annaolsen/Desktop/Speciale/DS_thesis/data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Load datasets\n",
    "df_bio = pd.read_csv(\"Tara_Biodiversity.csv\")\n",
    "df_meso = pd.read_csv(\"Tara_Env_Meso_SampleLocation.csv\")\n",
    "df_nutri = pd.read_csv(\"Tara_Env_Nut.csv\")\n",
    "\n",
    "# New column names for Meso SL\n",
    "new_column_names = {\n",
    "    'Sample ID (TARA_barcode#, registered at ...)': 'Sample ID (TARA_barcode#)',\n",
    "    'Station (TARA_station#, registered at ...)': 'Station'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "df_meso = df_meso.rename(columns=new_column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample IDs\n",
    "Remove unique sample IDs from BIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IDs bio: 73\n",
      "Unique IDs meso: 81\n",
      "Unique IDs nutri: 0\n",
      "\n",
      "Original shape of bio: (34768, 19)\n",
      "Original shape of meso: (34776, 71)\n",
      "Original shape of nutri: (34776, 38)\n",
      "\n",
      "Shape of unique bio df: (73, 19)\n",
      "Shape of unique meso df: (81, 71)\n",
      "Shape (bio) after removing unique IDs: (34695, 19)\n"
     ]
    }
   ],
   "source": [
    "# Find unique sample IDs\n",
    "ids_df_bio = set(df_bio[\"Sample ID (TARA_barcode#)\"])\n",
    "ids_df_meso = set(df_meso[\"Sample ID (TARA_barcode#)\"])\n",
    "ids_df_nutri = set(df_nutri['Sample ID'])\n",
    "\n",
    "# Find the Sample IDs that are not common between the DataFrames\n",
    "unique_ids_bio = ids_df_bio - ids_df_meso\n",
    "unique_ids_meso = ids_df_meso - ids_df_bio\n",
    "unique_ids_nutri = ids_df_nutri - ids_df_meso\n",
    "\n",
    "print(\"Unique IDs bio:\", len(unique_ids_bio))\n",
    "print(\"Unique IDs meso:\", len(unique_ids_meso))\n",
    "print(\"Unique IDs nutri:\", len(unique_ids_nutri))\n",
    "print()\n",
    "print(\"Original shape of bio:\", df_bio.shape)\n",
    "print(\"Original shape of meso:\", df_meso.shape)\n",
    "print(\"Original shape of nutri:\", df_nutri.shape)\n",
    "print()\n",
    "\n",
    "# Filter unique IDs from df_bio\n",
    "unique_df_bio = df_bio[df_bio[\"Sample ID (TARA_barcode#)\"].isin(\n",
    "    unique_ids_bio)]\n",
    "\n",
    "# Filter unique IDs from df_meso\n",
    "unique_df_meso = df_meso[df_meso[\"Sample ID (TARA_barcode#)\"].isin(\n",
    "    unique_ids_meso)]\n",
    "\n",
    "# Display the lengths of the unique dataframes\n",
    "print(\"Shape of unique bio df:\", unique_df_bio.shape)\n",
    "print(\"Shape of unique meso df:\", unique_df_meso.shape)\n",
    "\n",
    "# Create a boolean mask where IDs are in the list of IDs to remove\n",
    "mask = df_bio['Sample ID (TARA_barcode#)'].isin(unique_ids_bio)\n",
    "\n",
    "# Invert the mask to keep the rows that are not in the list of IDs to remove\n",
    "df_bio = df_bio[~mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Shape (bio) after removing unique IDs:\", df_bio.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Bio and Meso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging bio and meso: (34776, 84)\n"
     ]
    }
   ],
   "source": [
    "df_bio = df_bio.sort_values(by='Sample ID (TARA_barcode#)')\n",
    "df_meso = df_meso.sort_values(by='Sample ID (TARA_barcode#)')\n",
    "\n",
    "columns_to_drop = ['Campaign', 'Station (TARA_station#)', 'Event',\n",
    "                   'Env feature (abbreviation)',\n",
    "                   'Env feature (full name (ENVO:ID), terms re...)'\n",
    "                   ]\n",
    "\n",
    "df_bio = df_bio.drop(columns=columns_to_drop)\n",
    "\n",
    "# Columns we don't want to duplicate\n",
    "duplicate_cols = ['Sample ID (TARA_barcode#)']\n",
    "\n",
    "# Merge df_bio and df_meso\n",
    "df_merged = pd.merge(df_meso, df_bio, on=duplicate_cols, how=\"outer\")\n",
    "\n",
    "print(\"Shape after merging bio and meso:\", df_merged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Bio-Meso with Nutri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging bio/meso with nutri: (34776, 89)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop_nutri = [\n",
    "    'Sample ID (Bio)', 'Sample ID (ENA)', 'Station', 'Event',\n",
    "    'Env feature', 'Depth nominal', 'Depth top/min', 'Depth bot/max',\n",
    "    'Size frac lower', 'Size frac upper', 'Sample material',\n",
    "    'Sample method', 'Sample code/label', 'File name', 'Distance',\n",
    "    'Duration(ISO)', 'Number of observations', 'Nitrite min',\n",
    "    'Nitrite lower', 'Nitrite median', 'Nitrite upper', 'Nitrite max',\n",
    "    'Nitrate/Nitrite min', 'Nitrate/Nitrite lower', 'Nitrate/Nitrite median',\n",
    "    'Nitrate/Nitrite upper', 'Nitrate/Nitrite max', 'Silicate min',\n",
    "    'Silicate lower', 'Silicate median', 'Silicate upper', 'Silicate max'\n",
    "]\n",
    "\n",
    "df_nutri = df_nutri.drop(columns=cols_to_drop_nutri)\n",
    "\n",
    "\n",
    "# Rename the columns\n",
    "df_merged = df_merged.rename(\n",
    "    columns={'Sample ID (TARA_barcode#)': 'Sample ID'})\n",
    "\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_nutri, on='Sample ID', how=\"inner\")\n",
    "\n",
    "print(\"Shape after merging bio/meso with nutri:\", df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env feature ([abbreviation], full name (EN...)\n",
      "[SRF] surface water layer (ENVO:00010504)                                               16736\n",
      "[DCM] deep chlorophyll maximum layer (ENVO:01000326)                                     8476\n",
      "[ZZZ] marine water layer (ENVO:01000295)                                                 5347\n",
      "[MES] marine water layer (ENVO:01000295) within the mesopelagic zone (ENVO:00000213)     3271\n",
      "[MIX] marine epipelagic wind mixed layer (ENVO:01000061)                                  921\n",
      "unknown                                                                                    13\n",
      "[FSW] filtered sea water, used to control protocols                                        12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_merged['Env feature ([abbreviation], full name (EN...)'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/annaolsen/Desktop/Speciale/DS_thesis/data\n",
      "(34776, 77)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Load merged dataset\n",
    "# df = pd.read_csv(\"Tara_merged_BMN.csv\")\n",
    "\n",
    "\n",
    "# --------- Column names ---------\n",
    "df = df_merged.copy()\n",
    "\n",
    "# List of new column names\n",
    "new_column_names = [\n",
    "    'Sample ID', 'Sample ID (Bio)', 'Sample ID (ENA)', 'Basis', 'Campaign',\n",
    "    'Station', 'Method/Device', 'Event', 'Date/Time', 'Latitude', 'Longitude',\n",
    "    'Depth Layer Zone', 'Depth nominal', 'Depth top', 'Depth bot',\n",
    "    'Frac lower', 'Frac upper', 'Sample material', 'Sample method',\n",
    "    'Sample label', 'MP biome', 'OS region', 'BG province',\n",
    "    'Sea ice conc', 'Sea ice free start', 'Sea ice free days',\n",
    "    'Sea ice free end', 'Season', 'Season (early)',\n",
    "    'Moon phase nom', 'Moon phase prop', 'Time of day', 'Sunshine duration',\n",
    "    'Radiation', 'Radiation 8.1', 'Radiation 8.2', 'Radiation 30',\n",
    "    'Sea Surface Temp', 'Iron', 'Iron std', 'Ammonium', 'Ammonium std',\n",
    "    'Nitrite', 'Nitrite std', 'Nitrate', 'Nitrate std',\n",
    "    'Sea surface fluorescence', 'Sea surface chlorophyll a',\n",
    "    'Chlorophyll a', 'Sea surface quantum fluorescence',\n",
    "    'Net PP carbon', 'Net PP carbon 30', 'Total suspended matter',\n",
    "    'Particulate Organic Carbon', 'Particulate Inorganic Carbon',\n",
    "    'Depth bathy', 'Latitude.1', 'Longitude.1', 'Distance closest',\n",
    "    'Sea surface temp grad', 'Strain sub-mesoscale', 'u', 'v',\n",
    "    'Okubo-Weiss', 'Max Lyapunov Exp', 'Residence time',\n",
    "    'Latitude.2', 'Longitude.2', 'Age', 'Latitude.3', 'Longitude.3',\n",
    "    'Shannon_Darwin_mean_all', 'Shannon_Darwin_month_all',\n",
    "    'Shannon_Darwin_mean_grp', 'Shannon_Darwin_month_grp',\n",
    "    'Shannon_Physat_month', 'Shannon_Physat_mean', 'SILVA_Chao', 'SILVA_ace',\n",
    "    'SILVA_Shannon', 'SILVA_species_rich', 'SILVA_func_diversity',\n",
    "    'Functional richness', 'Functional evenness', 'Phosphate min', 'Phosphate lower',\n",
    "    'Phosphate median', 'Phosphate upper', 'Phosphate max'\n",
    "]\n",
    "\n",
    "# Change column names to list of new column names\n",
    "df.columns = new_column_names\n",
    "\n",
    "# --------- Add columns ---------\n",
    "\n",
    "# Convert the 'Date/Time' column to datetime format\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "\n",
    "# Extract Date and Year into new columns\n",
    "df['Date'] = df['Date/Time'].dt.date\n",
    "df['Year'] = df['Date/Time'].dt.year\n",
    "\n",
    "# Function to extract substrings within brackets [MES], [SRF] etc.\n",
    "def extract_code(value):\n",
    "    match = re.search(r'\\[([A-Z]+)\\]', value)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Use function to make new column with Depth Layer abbreviations \n",
    "df['Depth Layer'] = df['Depth Layer Zone'].apply(extract_code)\n",
    "\n",
    "\n",
    "# --------- Data cleaning ---------\n",
    "\n",
    "# Remove the ID pattern from columns - Example: (MRGID:1905)\n",
    "ID_pattern = r'\\(MRGID:(\\d+)\\)'\n",
    "\n",
    "df['OS region'] = df['OS region'].str.replace(ID_pattern, '', regex=True)\n",
    "df['BG province'] = df['BG province'].str.replace(ID_pattern, '', regex=True)\n",
    "\n",
    "\n",
    "# Remove the ENVO pattern from column - Example: (ENVO:00010504)\n",
    "ENVO_pattern = r'\\(ENVO:(\\d+)\\)'\n",
    "\n",
    "df['Depth Layer Zone'] = df['Depth Layer Zone'].str.replace(\n",
    "    ENVO_pattern, '', regex=True)\n",
    "\n",
    "# Remove the text in brackets from columns - Examples: [SRF] and [MS]\n",
    "bracket_columns = ['Depth Layer Zone', 'BG province', 'OS region']\n",
    "\n",
    "for col in bracket_columns:\n",
    "    df[col] = df[col].str.split(\"] \", expand=True)[1]  # Keep text after \"] \"\n",
    "    df[col] = df[col].str.strip()  # Strip for trailing whitespace\n",
    "\n",
    "\n",
    "# Replace values in Depth Layer Zone ('original': 'replace with')\n",
    "depth_layer_replace = {\n",
    "    'surface water layer': 'Surface Water',\n",
    "    'deep chlorophyll maximum layer': 'Deep Chlorophyll Maximum',\n",
    "    'marine water layer': 'Marine Water',\n",
    "    'marine water layer  within the mesopelagic zone': 'Marine Water Mesopelagic Zone',\n",
    "    'marine epipelagic wind mixed layer': 'Marine Epipelagic Wind Mixed',\n",
    "    'filtered sea water, used to control protocols': 'Filtered Sea Water',\n",
    "}\n",
    "\n",
    "# Apply replacements to Depth Layer Zone\n",
    "df['Depth Layer Zone'] = df['Depth Layer Zone'].replace(depth_layer_replace)\n",
    "\n",
    "\n",
    "# --------- Drop columns and sort dataframe ---------\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Age',\n",
    "    'Sample ID (Bio)', 'Sample ID (ENA)', \n",
    "    'Season (early)', 'Moon phase nom', 'Moon phase prop',\n",
    "    'Latitude.1', 'Longitude.1', 'Distance closest',\n",
    "    'Latitude.2', 'Longitude.2', 'Latitude.3', 'Longitude.3',\n",
    "    'Phosphate lower', 'Phosphate upper']\n",
    "\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Sort values\n",
    "df = df.sort_values(by='Date/Time')\n",
    "\n",
    "# Sort all columns (but Sample ID) alphabetically\n",
    "columns_to_sort = sorted(\n",
    "    [col for col in df.columns if col not in ['Sample ID']])\n",
    "\n",
    "# Reorder the columns\n",
    "desired_order = ['Sample ID'] + columns_to_sort\n",
    "\n",
    "# Create a new dataframe with the desired column order, sorted alphabetically\n",
    "df = df[desired_order]\n",
    "\n",
    "# Save dataframe to a new CSV file\n",
    "df.to_csv(\"Tara_BMN_Cleaned.csv\", index=False)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (34776, 77)\n",
      "Unique DataFrame: (4486, 77)\n",
      "Unique DataFrame with Latitude and Longitude: (4474, 77)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = pd.read_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/Tara_BMN_Cleaned.csv\")\n",
    "\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "\n",
    "columns_remove = [\n",
    "    'Sample ID', \n",
    "    'Frac lower',\n",
    "    'Frac upper',\n",
    "    'Sample label', \n",
    "    'Sample method',\n",
    "    'Sample material', \n",
    "\n",
    "    # 'Date/Time', \n",
    "    # 'Depth Layer Zone',\n",
    "    # 'Depth bot', 'Depth nominal', 'Depth top',\n",
    "    # 'Duration (ISO)', \n",
    "    # 'Event', \n",
    "    # 'Method/Device', \n",
    "\n",
    "    ]\n",
    "\n",
    "# # List of columns to consider when checking for duplicates\n",
    "columns_to_check = [col for col in df.columns if col not in columns_remove]\n",
    "\n",
    "# Drop duplicates based on specified columns\n",
    "df_unique = df.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "print(f\"Unique DataFrame: {df_unique.shape}\")\n",
    "\n",
    "# Drop rows without values in \"Latitude\" and \"Longitude\"\n",
    "df_unique = df_unique.dropna(subset=['Longitude', 'Latitude'])\n",
    "\n",
    "# Sort values and reset index\n",
    "df_unique = df_unique.sort_values(by='Date/Time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Unique DataFrame with Latitude and Longitude: {df_unique.shape}\")\n",
    "\n",
    "df_unique.to_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/TARA_Dash.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique rows in bio/depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging bio and depth: (34768, 76)\n",
      "Number of unique rows: 2921\n"
     ]
    }
   ],
   "source": [
    "df_bio = pd.read_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/Tara_Biodiversity.csv\")\n",
    "df_depth = pd.read_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/Tara_Environmental_Depth.csv\")\n",
    "\n",
    "# print(df_bio.shape)\n",
    "# print(df_depth.shape)\n",
    "\n",
    "# Columns we don't want to duplicate\n",
    "duplicate_cols = ['Sample ID (TARA_barcode#)', 'Campaign',\n",
    "                  'Station (TARA_station#)', 'Event',\n",
    "                  'Env feature (abbreviation)',\n",
    "                  'Env feature (full name (ENVO:ID), terms re...)']\n",
    "\n",
    "# Merge df_bio and df_depth\n",
    "df_merged = pd.merge(df_bio, df_depth, on=duplicate_cols, how=\"inner\")\n",
    "\n",
    "print(\"Shape after merging bio and depth:\", df_merged.shape)\n",
    "\n",
    "\n",
    "columns_remove = ['Sample ID (TARA_barcode#)']\n",
    "\n",
    "# List of columns to consider when checking for duplicates\n",
    "columns_to_check = [col for col in df_merged.columns if col not in columns_remove]\n",
    "\n",
    "# Drop duplicates based on specified columns\n",
    "unique_rows = df_merged.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "# Count the number of unique rows\n",
    "num_unique_rows = len(unique_rows)\n",
    "\n",
    "print(\"Number of unique rows:\", num_unique_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data (TARA_mhws_Dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4474, 79)\n",
      "(4474, 69)\n",
      "Index(['Sample ID', 'Ammonium', 'Ammonium std', 'BG province', 'Basis',\n",
      "       'Campaign', 'Chlorophyll a', 'Date/Time', 'Depth Layer Zone',\n",
      "       'Depth bathy', 'Depth bot', 'Depth nominal', 'Depth top', 'Event',\n",
      "       'Functional evenness', 'Functional richness', 'Iron', 'Iron std',\n",
      "       'Latitude', 'Longitude', 'MP biome', 'Max Lyapunov Exp',\n",
      "       'Method/Device', 'NPP Carbon', 'NPP Carbon 30', 'Nitrate',\n",
      "       'Nitrate std', 'Nitrite', 'Nitrite std', 'OS region', 'Okubo-Weiss',\n",
      "       'Particulate Inorganic Carbon', 'Particulate Organic Carbon',\n",
      "       'Phosphate max', 'Phosphate median', 'Phosphate min', 'Radiation',\n",
      "       'Radiation 30', 'Radiation 8.1', 'Radiation 8.2', 'Residence time',\n",
      "       'SILVA_Chao', 'SILVA_Shannon', 'SILVA_ace', 'SILVA_func_diversity',\n",
      "       'SILVA_species_rich', 'Sea Surface Temp', 'Sea ice conc',\n",
      "       'Sea ice free days', 'Sea ice free end', 'Sea ice free start',\n",
      "       'SS Chlorophyll a', 'SS Fluorescence', 'SS Quantum Fluorescence',\n",
      "       'SS Temp Grad', 'Season', 'Shannon_D_mean_all', 'Shannon_D_mean_grp',\n",
      "       'Shannon_D_month_all', 'Shannon_D_month_grp', 'Shannon_P_mean',\n",
      "       'Shannon_P_month', 'Station', 'Strain sub-mesoscale',\n",
      "       'Sunshine duration', 'Time of day', 'Total suspended matter', 'u', 'v'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/TARA_mhws_Dash_2.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "cols_to_drop = [\n",
    "    # 'Sample ID', \n",
    "    'Frac lower',\n",
    "    'Frac upper',\n",
    "    'Sample label', \n",
    "    'Sample method',\n",
    "    'Sample material', \n",
    "    'Date',\n",
    "    'Year',\n",
    "    'Depth Layer', \n",
    "    'MHW_count', 'MHW_category',\n",
    "    ]\n",
    "\n",
    "df_dt = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# print(df_dt.columns)\n",
    "\n",
    "print(df_dt.shape)\n",
    "\n",
    "\n",
    "new_column_names = [\n",
    "    'Sample ID', \n",
    "    'Ammonium', 'Ammonium std', 'BG province', 'Basis', 'Campaign',\n",
    "       'Chlorophyll a', 'Date/Time', 'Depth Layer Zone',\n",
    "       'Depth bathy', 'Depth bot', 'Depth nominal', 'Depth top', 'Event',\n",
    "       'Functional evenness', 'Functional richness', 'Iron', 'Iron std',\n",
    "       'Latitude', 'Longitude', 'MP biome', 'Max Lyapunov Exp',\n",
    "       'Method/Device', 'NPP Carbon', 'NPP Carbon 30', 'Nitrate',\n",
    "       'Nitrate std', 'Nitrite', 'Nitrite std', 'OS region', 'Okubo-Weiss',\n",
    "       'Particulate Inorganic Carbon', 'Particulate Organic Carbon',\n",
    "       'Phosphate max', 'Phosphate median', 'Phosphate min', 'Radiation',\n",
    "       'Radiation 30', 'Radiation 8.1', 'Radiation 8.2', 'Residence time',\n",
    "       'SILVA_Chao', 'SILVA_Shannon', 'SILVA_ace', 'SILVA_func_diversity',\n",
    "       'SILVA_species_rich', 'Sea Surface Temp', 'Sea ice conc',\n",
    "       'Sea ice free days', 'Sea ice free end', 'Sea ice free start',\n",
    "       'SS Chlorophyll a', 'SS Fluorescence',\n",
    "       'SS Quantum Fluorescence', 'SS Temp Grad', 'Season',\n",
    "       'Shannon_D_mean_all', 'Shannon_D_mean_grp',\n",
    "       'Shannon_D_month_all', 'Shannon_D_month_grp',\n",
    "       'Shannon_P_mean', 'Shannon_P_month', 'Station',\n",
    "       'Strain sub-mesoscale', 'Sunshine duration', 'Time of day',\n",
    "       'Total suspended matter', 'u', 'v',\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "df_dt.columns = new_column_names\n",
    "\n",
    "print(df_dt.columns)\n",
    "\n",
    "df_dt.to_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/TARA_BMN_thesis.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "Index(['Sample ID', 'Ammonium', 'Ammonium std', 'BG province', 'Basis',\n",
      "       'Campaign', 'Chl a', 'Date', 'Date/Time', 'Depth Layer',\n",
      "       'Depth Layer Zone', 'Depth bathy', 'Depth bot', 'Depth nom',\n",
      "       'Depth top', 'Event', 'Frac lower', 'Frac upper', 'Functional evenness',\n",
      "       'Functional richness', 'Iron', 'Iron std', 'Latitude', 'Longitude',\n",
      "       'MP biome', 'Max Lyapunov Exp', 'Method/Device', 'NPP Carbon',\n",
      "       'NPP Carbon 30', 'Nitrate', 'Nitrate std', 'Nitrite', 'Nitrite std',\n",
      "       'OS region', 'Okubo-Weiss', 'PIC', 'POC', 'Phosphate max',\n",
      "       'Phosphate med', 'Phosphate min', 'Radiation', 'Radiation 30',\n",
      "       'Radiation 8.1', 'Radiation 8.2', 'Residence time', 'SILVA_Chao',\n",
      "       'SILVA_Shannon', 'SILVA_ace', 'SILVA_func_diversity',\n",
      "       'SILVA_species_rich', 'Sample label', 'Sample material',\n",
      "       'Sample method', 'SST', 'Sea ice conc', 'Sea ice free days',\n",
      "       'Sea ice free end', 'Sea ice free start', 'SS Chl a', 'SS Fluorescence',\n",
      "       'SS Quantum Fluorescence', 'SST Grad', 'Season',\n",
      "       'Shannon_Darwin_mean_all', 'Shannon_Darwin_mean_grp',\n",
      "       'Shannon_Darwin_month_all', 'Shannon_Darwin_month_grp',\n",
      "       'Shannon_Physat_mean', 'Shannon_Physat_month', 'Station',\n",
      "       'Strain sub-mesoscale', 'Sunshine duration', 'Time of day', 'TSM',\n",
      "       'Year', 'u', 'v', 'MHW_count', 'MHW_category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/TARA_mhws_Dash_2.csv\")\n",
    "\n",
    "new_column_names = [\n",
    "    'Sample ID', 'Ammonium', 'Ammonium std', 'BG province', 'Basis',\n",
    "       'Campaign', 'Chl a', 'Date', 'Date/Time', 'Depth Layer',\n",
    "       'Depth Layer Zone', 'Depth bathy', 'Depth bot', 'Depth nom',\n",
    "       'Depth top', 'Event', 'Frac lower', 'Frac upper', 'Functional evenness',\n",
    "       'Functional richness', 'Iron', 'Iron std', 'Latitude', 'Longitude',\n",
    "       'MP biome', 'Max Lyapunov Exp', 'Method/Device', 'NPP Carbon',\n",
    "       'NPP Carbon 30', 'Nitrate', 'Nitrate std', 'Nitrite', 'Nitrite std',\n",
    "       'OS region', 'Okubo-Weiss', 'PIC', 'POC', 'Phosphate max', 'Phosphate med',\n",
    "       'Phosphate min', 'Radiation', 'Radiation 30', 'Radiation 8.1',\n",
    "       'Radiation 8.2', 'Residence time', 'SILVA_Chao', 'SILVA_Shannon',\n",
    "       'SILVA_ace', 'SILVA_func_diversity', 'SILVA_species_rich',\n",
    "       'Sample label', 'Sample material', 'Sample method', 'SST',\n",
    "       'Sea ice conc', 'Sea ice free days', 'Sea ice free end',\n",
    "       'Sea ice free start', 'SS Chl a', \n",
    "       'SS Fluorescence', 'SS Quantum Fluorescence',\n",
    "       'SST Grad', 'Season', 'Shannon_Darwin_mean_all',\n",
    "       'Shannon_Darwin_mean_grp', 'Shannon_Darwin_month_all',\n",
    "       'Shannon_Darwin_month_grp', 'Shannon_Physat_mean',\n",
    "       'Shannon_Physat_month', 'Station', 'Strain sub-mesoscale',\n",
    "       'Sunshine duration', 'Time of day', 'TSM', 'Year',\n",
    "       'u', 'v', 'MHW_count', 'MHW_category']\n",
    "\n",
    "\n",
    "\n",
    "new_column_names2 = [\n",
    "    'Sample ID', 'Ammonium', 'Ammonium std', 'BG province', 'Basis',\n",
    "       'Campaign', 'Chlorophyll a', 'Date', 'Date/Time', 'Depth Layer',\n",
    "       'Depth Layer Zone', 'Depth bathy', 'Depth bot', 'Depth nominal',\n",
    "       'Depth top', 'Event', 'Frac lower', 'Frac upper', 'Functional evenness',\n",
    "       'Functional richness', 'Iron', 'Iron std', 'Latitude', 'Longitude',\n",
    "       'MP biome', 'Max Lyapunov Exp', 'Method/Device', 'Net PP carbon',\n",
    "       'Net PP carbon 30', 'Nitrate', 'Nitrate std', 'Nitrite', 'Nitrite std',\n",
    "       'OS region', 'Okubo-Weiss', 'Particulate Inorganic Carbon',\n",
    "       'Particulate Organic Carbon', 'Phosphate max', 'Phosphate median',\n",
    "       'Phosphate min', 'Radiation', 'Radiation 30', 'Radiation 8.1',\n",
    "       'Radiation 8.2', 'Residence time', 'SILVA_Chao', 'SILVA_Shannon',\n",
    "       'SILVA_ace', 'SILVA_func_diversity', 'SILVA_species_rich',\n",
    "       'Sample label', 'Sample material', 'Sample method', 'Sea Surface Temp',\n",
    "       'Sea ice conc', 'Sea ice free days', 'Sea ice free end',\n",
    "       'Sea ice free start', 'Sea surface chlorophyll a',\n",
    "       'Sea surface fluorescence', 'Sea surface quantum fluorescence',\n",
    "       'Sea surface temp grad', 'Season', 'Shannon_Darwin_mean_all',\n",
    "       'Shannon_Darwin_mean_grp', 'Shannon_Darwin_month_all',\n",
    "       'Shannon_Darwin_month_grp', 'Shannon_Physat_mean',\n",
    "       'Shannon_Physat_month', 'Station', 'Strain sub-mesoscale',\n",
    "       'Sunshine duration', 'Time of day', 'Total suspended matter', 'Year',\n",
    "       'u', 'v', 'MHW_count', 'MHW_category']\n",
    "\n",
    "print(len(new_column_names))\n",
    "print(len(new_column_names2))\n",
    "\n",
    "\n",
    "df.columns = new_column_names\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df.to_csv(\"/Users/annaolsen/Desktop/Speciale/DS_thesis/data/TARA_mhws_Dash_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualizeF23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
